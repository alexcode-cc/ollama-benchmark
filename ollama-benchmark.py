import requests
import time
import json
from datetime import datetime

OLLAMA_BASE_URL = "http://localhost:11434"


BENCHMARK_PROMPTS = [
    {
        "name": "greeting",
        "prompt": "ä½ å¥½ï¼Œè«‹ä»¥ç¹é«”ä¸­æ–‡ç°¡å–®è‡ªæˆ‘ä»‹ç´¹ã€‚"
    },
    {
        "name": "reasoning",
        "prompt": "å¦‚æœä¸€å€‹æˆ¿é–“è£¡æœ‰ 3 å€‹äººï¼Œæ¯å€‹äººå„é¤Š 2 éš»è²“ï¼Œè«‹å•æˆ¿é–“è£¡ç¸½å…±æœ‰å¹¾éš»è…³ï¼Ÿè«‹èªªæ˜ç†ç”±ã€‚"
    },
    {
        "name": "coding",
        "prompt": "è«‹ç”¨ Python å¯«ä¸€å€‹å‡½å¼ï¼Œåˆ¤æ–·ä¸€å€‹æ•¸å­—æ˜¯å¦ç‚ºè³ªæ•¸ã€‚"
    },
    {
        "name": "expression",
        "prompt": "è«‹ç”¨ç¹é«”ä¸­æ–‡è§£é‡‹ä»€éº¼æ˜¯ RESTful APIï¼Œå°è±¡æ˜¯å‡è¨­å®Œå…¨æ²’æœ‰æŠ€è¡“èƒŒæ™¯çš„äººã€‚"
    },
]


def get_available_models():
    resp = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=30)
    resp.raise_for_status()
    data = resp.json()
    return [m["name"] for m in data.get("models", [])]


def ollama_generate(model: str, prompt: str):
    start = time.time()

    resp = requests.post(
        f"{OLLAMA_BASE_URL}/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": False,
        },
        timeout=600,
    )

    latency = round(time.time() - start, 3)

    resp.raise_for_status()
    data = resp.json()
    text = data.get("response", "")

    return {
        "response": text,
        "latency": latency,
        "length": len(text),
    }


def run_benchmark_for_model(model: str):
    print("=" * 70)
    print(f"ğŸ Benchmark é–‹å§‹ï¼š{model}")
    print("=" * 70)

    results = []

    for item in BENCHMARK_PROMPTS:
        print(f"â–¶ æ¸¬è©¦é …ç›®ï¼š{item['name']}")
        try:
            result = ollama_generate(model, item["prompt"])
            print(f"  â± {result['latency']}s | ğŸ“ {result['length']} chars")
            results.append({
                "test": item["name"],
                "prompt": item["prompt"],
                **result,
                "success": True,
            })
        except Exception as e:
            print(f"  âŒ å¤±æ•—ï¼š{e}")
            results.append({
                "test": item["name"],
                "prompt": item["prompt"],
                "response": "",
                "latency": None,
                "length": 0,
                "success": False,
                "error": str(e),
            })

    return results


def interactive_chat(model: str):
    print("\nğŸ’¬ é€²å…¥äººå·¥äº’å‹•æ¨¡å¼ï¼ˆEnter / n çµæŸï¼‰")
    while True:
        user_input = input("ä½ ï¼š").strip()
        if user_input.lower() in ("n", "no", ""):
            break
        reply = ollama_generate(model, user_input)
        print(f"{model}ï¼š\n{reply['response']}\n")


def main():
    models = get_available_models()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    report = {
        "generated_at": timestamp,
        "models": {},
    }

    print("ğŸ“¦ åµæ¸¬åˆ°æ¨¡å‹ï¼š")
    for m in models:
        print(f" - {m}")
    print()

    for model in models:
        benchmark_results = run_benchmark_for_model(model)
        report["models"][model] = {
            "benchmark": benchmark_results
        }

        choice = input("\næ˜¯å¦è¦èˆ‡æ­¤æ¨¡å‹äº’å‹•ï¼Ÿ(y/N)ï¼š ").strip().lower()
        if choice == "y":
            interactive_chat(model)

    output_file = f"ollama_benchmark_{timestamp}.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… Benchmark å®Œæˆï¼Œå ±å‘Šå·²è¼¸å‡ºï¼š{output_file}")


if __name__ == "__main__":
    main()
