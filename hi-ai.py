import requests

OLLAMA_BASE_URL = "http://localhost:11434"


def get_available_models():
    """å¾ Ollama ä¼ºæœå™¨å–å¾—ç›®å‰å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    resp = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=30)
    resp.raise_for_status()
    data = resp.json()
    return [m["name"] for m in data.get("models", [])]


def llama_local(prompt: str, model: str):
    """å‘¼å« Ollama ç”¢ç”Ÿå›æ‡‰"""
    resp = requests.post(
        f"{OLLAMA_BASE_URL}/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": False,
        },
        timeout=600,
    )
    resp.raise_for_status()
    data = resp.json()
    return data["response"]


def chat_with_model(model: str):
    """èˆ‡å–®ä¸€æ¨¡å‹äº’å‹•"""
    print("=" * 60)
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ï¼š{model}")
    print("=" * 60)

    # åˆå§‹æ‰“æ‹›å‘¼
    reply = llama_local("ä½ å¥½ï¼Œè«‹ä»¥ç¹é«”ä¸­æ–‡å‘æˆ‘æ‰“æ‹›å‘¼ä¸¦ç°¡å–®è‡ªæˆ‘ä»‹ç´¹ã€‚", model)
    print(f"{model}ï¼š\n{reply}\n")

    while True:
        user_input = input("ä½ è¦ç¹¼çºŒè·Ÿé€™å€‹æ¨¡å‹èŠå¤©å—ï¼Ÿ(ç›´æ¥è¼¸å…¥å…§å®¹ / n è·³åˆ°ä¸‹ä¸€å€‹æ¨¡å‹)ï¼š ").strip()
        if user_input.lower() in ("n", "no", "next", ""):
            print(f"â¡ï¸  åˆ‡æ›åˆ°ä¸‹ä¸€å€‹æ¨¡å‹ï¼š{model}\n")
            break

        reply = llama_local(user_input, model)
        print(f"{model}ï¼š\n{reply}\n")


def main():
    models = get_available_models()

    if not models:
        print("âš ï¸  Ollama ä¼ºæœå™¨ç›®å‰æ²’æœ‰ä»»ä½•å¯ç”¨æ¨¡å‹")
        return

    print("ğŸ“¦ åµæ¸¬åˆ°ä»¥ä¸‹å¯ç”¨æ¨¡å‹ï¼š")
    for m in models:
        print(f" - {m}")
    print()

    for model in models:
        chat_with_model(model)

    print("âœ… æ‰€æœ‰æ¨¡å‹æ¸¬è©¦å®Œæˆ")


if __name__ == "__main__":
    main()
