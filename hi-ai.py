import json
import requests

OLLAMA_BASE_URL = "http://localhost:11434"

GREETING_PROMPT = "ä½ æ˜¯èª°"
#GREETING_PROMPT = "ä½ å¥½ï¼Œè«‹ä»¥ç¹é«”ä¸­æ–‡å‘æˆ‘æ‰“æ‹›å‘¼ä¸¦ç°¡å–®è‡ªæˆ‘ä»‹ç´¹ã€‚"

GREETING_TIMEOUT_SECONDS = 30

# ç”¨æ–¼æ¯”å° Ollama å›å‚³çš„ OOM / è¨˜æ†¶é«”ç›¸é—œéŒ¯èª¤è¨Šæ¯
OOM_KEYWORDS = [
    "out of memory", "oom", "not enough memory",
    "failed to load", "insufficient memory",
    "cuda out of memory", "memory", "alloc",
]


def get_available_models() -> list[str]:
    """å¾ Ollama ä¼ºæœå™¨å–å¾—ç›®å‰æœ‰æä¾›æœå‹™çš„æ¨¡å‹åˆ—è¡¨"""
    resp = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=GREETING_TIMEOUT_SECONDS)
    resp.raise_for_status()
    data = resp.json()
    return [m["name"] for m in data.get("models", [])]


def get_running_models() -> list[dict]:
    """å–å¾—ç›®å‰å·²è¼‰å…¥è¨˜æ†¶é«”çš„æ¨¡å‹æ¸…å–®ï¼ˆé€é /api/psï¼‰"""
    resp = requests.get(f"{OLLAMA_BASE_URL}/api/ps", timeout=10)
    resp.raise_for_status()
    return resp.json().get("models", [])


def unload_model(model_name: str) -> bool:
    """å¸è¼‰æŒ‡å®šæ¨¡å‹ï¼Œé‡‹æ”¾å…¶ä½”ç”¨çš„è¨˜æ†¶é«”ã€‚å›å‚³æ˜¯å¦æˆåŠŸã€‚"""
    try:
        resp = requests.post(
            f"{OLLAMA_BASE_URL}/api/chat",
            json={
                "model": model_name,
                "messages": [],
                "keep_alive": 0,
            },
            timeout=GREETING_TIMEOUT_SECONDS,
        )
        resp.raise_for_status()
        return True
    except requests.RequestException as e:
        print(f"   âš ï¸  å¸è¼‰ {model_name} å¤±æ•—ï¼š{e}", flush=True)
        return False


def unload_all_models() -> None:
    """å¸è¼‰æ‰€æœ‰ç›®å‰å·²è¼‰å…¥è¨˜æ†¶é«”çš„æ¨¡å‹ï¼Œä»¥é‡‹æ”¾è¨˜æ†¶é«”ç©ºé–“ã€‚"""
    try:
        running = get_running_models()
    except requests.RequestException:
        return

    if not running:
        return

    names = [m.get("name", "unknown") for m in running]
    total_size = sum(m.get("size", 0) for m in running)
    print(
        f"ğŸ§¹ æ­£åœ¨å¸è¼‰ {len(running)} å€‹å·²è¼‰å…¥çš„æ¨¡å‹ä»¥é‡‹æ”¾è¨˜æ†¶é«”"
        f"ï¼ˆå…± {_format_bytes(total_size)}ï¼‰â€¦",
        flush=True,
    )
    for m in running:
        name = m.get("name", "")
        size = m.get("size", 0)
        if name:
            print(f"   å¸è¼‰ {name} ({_format_bytes(size)})â€¦", end="", flush=True)
            ok = unload_model(name)
            print(" âœ…" if ok else " âŒ", flush=True)


def _is_oom_error(error_msg: str) -> bool:
    """åˆ¤æ–·éŒ¯èª¤è¨Šæ¯æ˜¯å¦èˆ‡ OOM / è¨˜æ†¶é«”ä¸è¶³ç›¸é—œ"""
    lower = error_msg.lower()
    return any(kw in lower for kw in OOM_KEYWORDS)


def _format_bytes(n: int) -> str:
    """å°‡ä½å…ƒçµ„æ•¸æ ¼å¼åŒ–ç‚ºäººé¡å¯è®€çš„å­—ä¸²"""
    for unit in ("B", "KB", "MB", "GB", "TB"):
        if abs(n) < 1024:
            return f"{n:.1f} {unit}"
        n /= 1024
    return f"{n:.1f} PB"


def show_model_resource_usage(model: str) -> None:
    """é¡¯ç¤ºæŒ‡å®šæ¨¡å‹çš„è³‡æºä½”ç”¨æƒ…å½¢ï¼ˆé€é /api/psï¼‰"""
    try:
        running = get_running_models()
        for m in running:
            if m.get("name") == model or m.get("model") == model:
                size = m.get("size", 0)
                size_vram = m.get("size_vram", 0)
                
                if size > 0:
                    vram_pct = (size_vram / size) * 100 if size > 0 else 0
                    print(f"ğŸ“Š è³‡æºä½”ç”¨ï¼šæ¨¡å‹å¤§å° {_format_bytes(size)}ï¼ŒVRAM {_format_bytes(size_vram)} ({vram_pct:.1f}%)", flush=True)
                    
                    if size_vram < size * 0.95:  # æœªé” 95% è¡¨ç¤ºéƒ¨åˆ†åœ¨ç³»çµ±è¨˜æ†¶é«”
                        system_mem = size - size_vram
                        print(f"   âš ï¸  ç³»çµ±è¨˜æ†¶é«” {_format_bytes(system_mem)}ï¼ˆæ•ˆèƒ½å¯èƒ½ä¸‹é™ï¼‰", flush=True)
                else:
                    print(f"ğŸ“Š è³‡æºä½”ç”¨ï¼šæ¨¡å‹å·²è¼‰å…¥", flush=True)
                return
        
        # æ¨¡å‹ä¸åœ¨åŸ·è¡Œæ¸…å–®ä¸­
        print(f"ğŸ“Š è³‡æºä½”ç”¨ï¼šæ¨¡å‹è³‡è¨Šç„¡æ³•å–å¾—", flush=True)
    except requests.RequestException:
        # ç„¡æ³•é€£ç·š /api/psï¼Œéœé»˜è™•ç†
        pass


def diagnose_timeout(model: str, got_any_token: bool) -> str:
    """åœ¨é€¾æ™‚å¾Œï¼Œé€é /api/ps è¨ºæ–·å¯èƒ½åŸå› ä¸¦å›å‚³æè¿°å­—ä¸²ã€‚
    got_any_tokenï¼šåœ¨é€¾æ™‚å‰æ˜¯å¦å·²æ”¶åˆ°ä»»ä½•ç”Ÿæˆ tokenã€‚
    """
    diagnosis_parts: list[str] = []

    # éšæ®µåˆ¤æ–·
    if not got_any_token:
        diagnosis_parts.append("æ¨¡å‹åœ¨è¼‰å…¥éšæ®µå³é€¾æ™‚ï¼ˆå°šæœªç”¢ç”Ÿä»»ä½• tokenï¼‰")
    else:
        diagnosis_parts.append("æ¨¡å‹å·²é–‹å§‹ç”Ÿæˆä½†å›æ‡‰éæ…¢")

    # é€é /api/ps æŸ¥è©¢ç›®å‰è¼‰å…¥çš„æ¨¡å‹èˆ‡è¨˜æ†¶é«”ç‹€æ…‹
    try:
        ps_resp = requests.get(f"{OLLAMA_BASE_URL}/api/ps", timeout=5)
        ps_resp.raise_for_status()
        ps_data = ps_resp.json()
        running_models = ps_data.get("models", [])

        target_found = False
        for m in running_models:
            if m.get("name") == model or m.get("model") == model:
                target_found = True
                size = m.get("size", 0)
                size_vram = m.get("size_vram", 0)
                if size > 0:
                    vram_pct = (size_vram / size) * 100
                    diagnosis_parts.append(
                        f"æ¨¡å‹å¤§å° {_format_bytes(size)}ï¼Œ"
                        f"VRAM ä½¿ç”¨ {_format_bytes(size_vram)} ({vram_pct:.0f}%)"
                    )
                    if size_vram < size:
                        diagnosis_parts.append(
                            "âš ï¸  æ¨¡å‹æœªå®Œå…¨è¼‰å…¥ VRAMï¼Œéƒ¨åˆ†ä½¿ç”¨ç³»çµ±è¨˜æ†¶é«”ï¼Œæ•ˆèƒ½å¤§å¹…ä¸‹é™"
                        )
                break

        if not target_found and not got_any_token:
            diagnosis_parts.append("âš ï¸  æ¨¡å‹æœªå‡ºç¾åœ¨åŸ·è¡Œæ¸…å–®ä¸­ï¼Œå¾ˆå¯èƒ½å› è¨˜æ†¶é«”ä¸è¶³ (OOM) ç„¡æ³•è¼‰å…¥")
            # åˆ—å‡ºå…¶ä»–ä½”ç”¨è¨˜æ†¶é«”çš„æ¨¡å‹
            if running_models:
                others = [
                    f"{m.get('name', 'unknown')} ({_format_bytes(m.get('size', 0))})"
                    for m in running_models
                ]
                diagnosis_parts.append(f"ç›®å‰å·²è¼‰å…¥çš„æ¨¡å‹ï¼š{', '.join(others)}")

    except requests.RequestException:
        diagnosis_parts.append("ï¼ˆç„¡æ³•é€£ç·š /api/ps é€²è¡Œé€²ä¸€æ­¥è¨ºæ–·ï¼‰")

    return "ï¼›".join(diagnosis_parts)


class OllamaError(Exception):
    """Ollama API å›å‚³çš„éŒ¯èª¤"""


def llama_local(prompt: str, model: str, *, timeout: int = GREETING_TIMEOUT_SECONDS*60, show_resource: bool = True) -> str:
    """å‘¼å« Ollama ç”¢ç”Ÿå›æ‡‰ï¼ˆä½¿ç”¨ streaming æ¨¡å¼ï¼‰ã€‚timeoutï¼šé€¾æ™‚ç§’æ•¸ï¼Œé è¨­ GREETING_TIMEOUT_SECONDS*60ã€‚
    show_resourceï¼šæ˜¯å¦åœ¨æ”¶åˆ°ç¬¬ä¸€å€‹ token å¾Œé¡¯ç¤ºè³‡æºä½”ç”¨ã€‚"""
    resp = requests.post(
        f"{OLLAMA_BASE_URL}/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": True,
        },
        stream=True,
        timeout=(10, timeout),  # (é€£ç·šé€¾æ™‚, è®€å–é€¾æ™‚â€”å…©æ¬¡è³‡æ–™ä¹‹é–“çš„æœ€å¤§ç­‰å¾…)
    )
    resp.raise_for_status()

    full_response: list[str] = []
    first_token_received = False
    
    for line in resp.iter_lines():
        if not line:
            continue
        chunk = json.loads(line)
        # Ollama ä¸²æµä¸­å›å‚³éŒ¯èª¤
        if "error" in chunk:
            raise OllamaError(chunk["error"])
        token = chunk.get("response", "")
        if token:
            # æ”¶åˆ°ç¬¬ä¸€å€‹ token æ™‚é¡¯ç¤ºè³‡æºä½”ç”¨
            if not first_token_received and show_resource:
                first_token_received = True
                show_model_resource_usage(model)
            full_response.append(token)
        if chunk.get("done"):
            break

    return "".join(full_response).strip() or "(ç„¡å›è¦†)"


def llama_local_greeting(prompt: str, model: str, *, timeout: int = GREETING_TIMEOUT_SECONDS) -> str:
    """å°ˆç‚ºæ‰“æ‹›å‘¼è¨­è¨ˆï¼šä½¿ç”¨ streaming æ¨¡å¼ï¼Œè¿½è¹¤æ˜¯å¦æ”¶åˆ° token ä»¥ä¾¿é€¾æ™‚è¨ºæ–·ã€‚"""
    got_any_token = False
    first_token_received = False
    
    try:
        resp = requests.post(
            f"{OLLAMA_BASE_URL}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": True,
            },
            stream=True,
            timeout=(10, timeout),
        )
        resp.raise_for_status()

        full_response: list[str] = []
        for line in resp.iter_lines():
            if not line:
                continue
            chunk = json.loads(line)
            if "error" in chunk:
                raise OllamaError(chunk["error"])
            token = chunk.get("response", "")
            if token:
                # æ”¶åˆ°ç¬¬ä¸€å€‹ token æ™‚é¡¯ç¤ºè³‡æºä½”ç”¨
                if not first_token_received:
                    first_token_received = True
                    show_model_resource_usage(model)
                got_any_token = True
                full_response.append(token)
            if chunk.get("done"):
                break

        return "".join(full_response).strip() or "(ç„¡å›è¦†)"

    except requests.Timeout:
        diag = diagnose_timeout(model, got_any_token)
        raise TimeoutWithDiagnosis(diag) from None

    except requests.HTTPError as e:
        # å˜—è©¦å¾å›æ‡‰å…§å®¹è§£æ OOM éŒ¯èª¤
        error_body = ""
        if e.response is not None:
            try:
                error_body = e.response.text
            except Exception:
                pass
        if error_body and _is_oom_error(error_body):
            raise OllamaError(f"è¨˜æ†¶é«”ä¸è¶³ (OOM)ï¼š{error_body}") from None
        raise


class TimeoutWithDiagnosis(Exception):
    """é€¾æ™‚ä¸”é™„å¸¶è¨ºæ–·è³‡è¨Š"""


def greeting_for_model(model: str) -> str | None:
    """å°å–®ä¸€æ¨¡å‹åŸ·è¡Œæ‰“æ‹›å‘¼æ¸¬è©¦ï¼Œè¶…é GREETING_TIMEOUT_SECONDS ç§’æœªå›æ‡‰å‰‡è·³éã€‚
    å›å‚³æ¨¡å‹å›è¦†æˆ– Noneï¼ˆå¤±æ•—/é€¾æ™‚æ™‚ï¼‰ã€‚"""
    print(f"â³ æ­£åœ¨å–å¾— {model} çš„æ‰“æ‹›å‘¼å›è¦†â€¦ï¼ˆé€¾æ™‚ {GREETING_TIMEOUT_SECONDS} ç§’ï¼‰", flush=True)
    try:
        reply = llama_local_greeting(GREETING_PROMPT, model, timeout=GREETING_TIMEOUT_SECONDS)
        return reply

    except TimeoutWithDiagnosis as e:
        print(f"â±ï¸  è¶…é {GREETING_TIMEOUT_SECONDS} ç§’æœªå®Œæˆå›æ‡‰ï¼Œè·³éæ­¤æ¨¡å‹ã€‚", flush=True)
        print(f"   è¨ºæ–·ï¼š{e}", flush=True)
        return None

    except OllamaError as e:
        error_msg = str(e)
        if _is_oom_error(error_msg):
            print(f"ğŸ’¥ è¨˜æ†¶é«”ä¸è¶³ (OOM)ï¼Œç„¡æ³•è¼‰å…¥æˆ–åŸ·è¡Œæ­¤æ¨¡å‹ï¼š{error_msg}", flush=True)
        else:
            print(f"âŒ Ollama éŒ¯èª¤ï¼š{error_msg}", flush=True)
        return None

    except requests.RequestException as e:
        print(f"âŒ å–å¾—å›è¦†å¤±æ•—ï¼š{e}", flush=True)
        return None


def chat_with_model(model: str) -> None:
    """å°å–®ä¸€æ¨¡å‹ï¼šå…ˆæ‰“æ‹›å‘¼ï¼Œå†è©¢å•æ˜¯å¦ç¹¼çºŒäº¤è«‡ï¼›ä¸ç¹¼çºŒå‰‡çµæŸæ­¤æ¨¡å‹æµç¨‹"""
    print("=" * 60, flush=True)
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ï¼š{model}", flush=True)
    print("=" * 60, flush=True)

    # å…ˆå¸è¼‰æ‰€æœ‰å·²è¼‰å…¥çš„æ¨¡å‹ï¼Œç¢ºä¿æœ‰è¶³å¤ è¨˜æ†¶é«”è¼‰å…¥æ–°æ¨¡å‹
    unload_all_models()

    # åŸ·è¡Œæ‰“æ‹›å‘¼æ¸¬è©¦
    reply = greeting_for_model(model)
    if reply is None:
        print("ç•¥éæ­¤æ¨¡å‹ï¼Œå‰å¾€ä¸‹ä¸€å€‹ã€‚\n", flush=True)
        return

    print(f"\n{model}ï¼š\n{reply}\n", flush=True)

    # æš«åœï¼šè®“ä½¿ç”¨è€…ç¢ºèªæ˜¯å¦ç¹¼çºŒèˆ‡æ­¤æ¨¡å‹äº¤è«‡
    while True:
        user_input = input("æ˜¯å¦ç¹¼çºŒèˆ‡æ­¤æ¨¡å‹äº¤è«‡ï¼Ÿ(y/Enter=ç¹¼çºŒ, n=ä¸‹ä¸€å€‹æ¨¡å‹, q=é›¢é–‹)ï¼š ").strip().lower()
        if user_input in ("q", "quit", "exit"):
            print("ğŸ‘‹ é›¢é–‹ç¨‹å¼", flush=True)
            exit(0)
        
        if user_input in ("n", "no", "next"):
            print(f"â¡ï¸  åˆ‡æ›åˆ°ä¸‹ä¸€å€‹æ¨¡å‹\n", flush=True)
            return

        if user_input in ("", "y", "yes"):
            break
        print("è«‹è¼¸å…¥ y ç¹¼çºŒã€n è·³åˆ°ä¸‹ä¸€å€‹æ¨¡å‹ï¼Œæˆ– q é›¢é–‹ç¨‹å¼ã€‚")

    # ç¹¼çºŒäº¤è«‡è¿´åœˆ
    while True:
        user_input = input("ä½ ï¼š ").strip()
        if user_input.lower() in ("n", "no", "next", "quit", "q"):
            print(f"â¡ï¸  åˆ‡æ›åˆ°ä¸‹ä¸€å€‹æ¨¡å‹\n", flush=True)
            return
        if not user_input:
            continue
        try:
            reply = llama_local(user_input, model)
            print(f"{model}ï¼š\n{reply}\n", flush=True)
        except requests.RequestException as e:
            print(f"âŒ è«‹æ±‚å¤±æ•—ï¼š{e}\n", flush=True)


def main():
    models = get_available_models()

    if not models:
        print("âš ï¸  Ollama ä¼ºæœå™¨ç›®å‰æ²’æœ‰ä»»ä½•å¯ç”¨æ¨¡å‹", flush=True)
        return

    print("ğŸ“¦ åµæ¸¬åˆ°ä»¥ä¸‹å¯ç”¨æ¨¡å‹ï¼š", flush=True)
    for m in models:
        print(f" - {m}", flush=True)
    print(flush=True)

    for model in models:
        chat_with_model(model)

    print("âœ… æ‰€æœ‰æ¨¡å‹æ¸¬è©¦å®Œæˆ", flush=True)


if __name__ == "__main__":
    main()
